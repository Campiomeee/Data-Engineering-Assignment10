# Data Engineering Assignment 10 â€“ Airflow ETL Pipeline
Data Engineering Assignment 10


# ğŸ“Œ Project Overview

This project demonstrates deploying Apache Airflow in a development environment and creating an automated ETL pipeline. The pipeline ingests data from multiple sources, performs transformations, merges the datasets, and loads the processed data into a PostgreSQL database. Finally, analytical tasks are performed on the transformed data, including visualization and cleanup of intermediate data.


## ğŸ“ Directory Structure
DATA-ENGINEERING-ASSIGNMENT10/
â”‚
â”œâ”€â”€ .devcontainer/ # Development container config
â”œâ”€â”€ airflow-env/ # Python virtual environment for Airflow
â”‚
â”œâ”€â”€ dags/
â”‚ â”œâ”€â”€ pipeline.py # Main DAG implementation
â”‚ â””â”€â”€ __pycache__/
â”‚
â”œâ”€â”€ data/ # Raw and processed datasets
â”œâ”€â”€ logs/ # Airflow logs
â”œâ”€â”€ pics/ # Visualization output (e.g., Pipeline.png)
â”œâ”€â”€ plugins/ # Custom Airflow plugins (if any)
â”‚
â”œâ”€â”€ airflow.cfg # Airflow configuration
â”œâ”€â”€ requirements.txt # Python dependencies
â””â”€â”€ README.md


DAG Graph

Output screenshot saved as /pics/Pipeline.png

![Pipeline DAG](pics/Pipeline.png)

# â–¶ï¸Instructions to Run

Start development container

Install dependencies

pip install -r requirements.txt

Initialize Airflow

airflow db init

Start Airflow

airflow scheduler & airflow webserver

Access Airflow UI: http://localhost:8080

Enable DAG pipeline